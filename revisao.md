# Fundamentos Matem√°ticos para Intelig√™ncia Artificial (25B4)

##  Revis√£o

### Parte 1. Fun√ß√µes

Calcular o valor de fun√ß√µes 

### **Exerc√≠cio 1**

A fun√ß√£o √©:

$f(x) = 3x^2 - 2x + 1$

Calcule os valores:

$f(0), \quad f(1), \quad f(2)$

---

### **Exerc√≠cio 2**

A fun√ß√£o √©:

$g(x) = 4^{-x} + \sin(x)$

Calcule os valores:

$g(0), \quad g\left(\frac{\pi}{2}\right), \quad g(1)$

---

### **Exerc√≠cio 3**

A fun√ß√£o √©:

$h(x) = \frac{x^3 - 4x}{x^2 + 1}$

Calcule os valores:

$h(0), \quad h(1), \quad h(2)$

Comente se a fun√ß√£o ( $h(x)$ ) tende a **crescer** ou **decrescer** para valores grandes de ( $x$ ).



### **Exerc√≠cio 4** - Identifique a fun√ß√£o que gerou o gr√°fico

| Figura 1 | Fun√ß√µes |
|-----------|-----------|
| ![F1](./plots/fig1.png) | ![E1](./plots/eq1.png) |
| ![F2](./plots/fig2.png) | ![E2](./plots/eq2.png) |
| ![F3](./plots/fig3.png) | ![E3](./plots/eq3.png) |

### Parte 2 - Dist√¢ncias e Similaridades


### **Exerc√≠cio 5**. Considere os pontos na reta real:

$A(1),\quad B(4),\quad C(7)$

| ![F4](./plots/fig4.png) 

- Calcule as dist√¢ncias euclidianas
   $d(A,B),\quad d(B,C)\quad\text{e}\quad d(A,C)$

- Verifique se essas dist√¢ncias satisfazem a desigualdade triangular:
   $d(A,C)\le d(A,B) + d(B,C)$

*(Use a f√≥rmula da dist√¢ncia euclidiana em 1D: $d(X,Y)=|x_X-x_Y|$*

2. Calcule as dist√¢ncias euclidianas entre os pares de pontos.

| ![F5](./plots/fig5.png) 

- Qual ponto est√° mais distante de ùëÉ?
- Considere os vetores que unem os pontos ùëÉQR. O √¢ngulo entre os vetores  PQ e PR √© pequeno ou grande?


Dadas duas palavras representadas por vetores em $R^3$: 
$ùë¢=[1,2,3]$ e $ùë£=[2,0,1]$. Calcule a similaridade por cosseno entre eles. Mostre todos os passos (produto interno, normas, divis√£o).



### **Exerc√≠cio 6** - Calcule a similaridade do cosseno entre 


![F6](./plots/fig6.png) 


Baseado no gr√°fico, o √¢ngulo entre os vetores √© pequeno ou grande?

Interprete o resultado: os vetores apontam para dire√ß√µes mais semelhantes ou diferentes?

# Parte 3 - Gradiente Descendente

Considere a fun√ß√£o de erro

$$
J(w) = (w - 3)^2 + 2
$$

O objetivo √© **encontrar o valor de (w)** que **minimiza (J(w))**, usando **gradiente descendente** com taxa de aprendizado (\eta = 0{,}1).
O gradiente √© dado por
[
\nabla J(w) = 2(w - 3)
]

---

## Parte 1 ‚Äî Observando o comportamento

A tabela abaixo mostra as 4 primeiras itera√ß√µes j√° calculadas:

| Itera√ß√£o | (w_t)  | (\nabla J(w_t)) | (J(w_t)) | Pr√≥ximo ($w_{t+1}$) (j√° calculado) |
| -------- | ------ | --------------- | -------- | -------------------------------- |
| 0        | 0.0000 | -6.0000         | 11.000   | 0.6000                           |
| 1        | 0.6000 | -4.8000         | 7.760    | 1.0800                           |
| 2        | 1.0800 | -3.8400         | 5.686    | 1.4640                           |
| 3        | 1.4640 | -3.0720         | 4.359    | 1.7712                           |

---

## Parte 2 ‚Äî Perguntas de interpreta√ß√£o

1. **Dire√ß√£o do movimento:**
   O gradiente √© negativo em todas as itera√ß√µes mostradas.
   **Pergunta:** o que isso nos diz sobre o comportamento da fun√ß√£o? Estamos nos movendo para a direita ou para a esquerda no eixo (w)?
   **Explique com suas palavras.**

2. **Tamanho do passo:**
   Observe que o valor absoluto do gradiente (|-6|, |-4.8|, |-3.84|, ‚Ä¶) est√° diminuindo.
   **Pergunta:** o que isso significa em termos de velocidade de aprendizado e aproxima√ß√£o do m√≠nimo?

3. **Tend√™ncia:**
   Note que (w_t) est√° se aproximando de 3.
   **Pergunta:** se continuarmos aplicando o mesmo processo, qual seria **aproximadamente** o pr√≥ximo valor (w_4)?
   (Dica: use a regra (w_{t+1} = w_t - 0{,}1 \times 2(w_t - 3)))

4. **Sentido da converg√™ncia:**
   Quando o gradiente ficar muito pr√≥ximo de zero, o que isso significa?
   **Pergunta:** o que aconteceria se a taxa de aprendizado fosse **0.5** em vez de **0.1**?

---

## Parte 3 ‚Äî Conclus√£o 

* O **sinal do gradiente** mostra **para onde andar** (positivo ‚Üí esquerda, negativo ‚Üí direita).
* O **m√≥dulo do gradiente** mostra **o quanto andar** (quanto mais perto do m√≠nimo, menor o passo).
* A **taxa de aprendizado** ajusta **a sensibilidade do movimento**:
  valores altos podem causar saltos inst√°veis, valores baixos tornam a descida lenta.

O ponto de equil√≠brio ocorre em (w = 3), onde ($\nabla J(w) = 0$).

A curva azul mostra a fun√ß√£o de erro (J(w)), e os pontos vermelhos representam as itera√ß√µes do gradiente descendente.

![](./plots/grad_desc.png)

* O primeiro ponto ((w_0 = 0)) est√° longe do m√≠nimo.
* A cada itera√ß√£o, os pontos se aproximam de (w = 3), o fundo da par√°bola, onde o gradiente √© zero.
* As setas (impl√≠citas pela sequ√™ncia de pontos) indicam que o algoritmo se move **da esquerda para a direita**, com **passos cada vez menores**, pois a inclina√ß√£o da curva vai diminuindo ‚Äî exatamente o comportamento esperado do gradiente descendente.


# Parte 4 - Simula√ß√£o de Monte Carlo


**Objetivo.** Estimar, por simula√ß√£o de Monte Carlo, a probabilidade de que a **soma de tr√™s dados justos**
(S = D_1 + D_2 + D_3) seja **maior ou igual a 12**.

**Defini√ß√£o do indicador.** Para cada rodada (i), defina
$$
I_i=\begin{cases}
1,& \text{se } S_i\ge 12\
0,& \text{caso contr√°rio}
\end{cases}
\quad\Rightarrow\quad
\hat p_n=\dfrac{1}{n}\sum_{i=1}^{n} I_i.
$$
Ou seja, (\hat p_n) √© a **estimativa corrente** de (P(S\ge 12)).

---

## Parte 1 ‚Äî Rodadas j√° simuladas (interpretar e prever)

A tabela abaixo mostra **10 rodadas** j√° simuladas. O valor ‚ÄúSoma‚Äù √© ($S_i$), ‚ÄúIndicador‚Äù √© ($I_i$), ‚ÄúAcum.‚Äù √© $\sum_{j=1}^{i} I_j$ e $\hat p_i$ √© a estimativa at√© a rodada ($i$).

| Rodada (i) | (D_1,D_2,D_3) | Soma ($S_i$) | Indicador ($I_i$) | Acum. ($\sum_{j=1}^{i} I_j$) | ($\hat p_i$) |
| ---------- | ------------- | ---------- | --------------- | -------------------------- | ---------- |
| 1          | 2, 5, 3       | 10         | 0               | 0                          | 0.000      |
| 2          | 6, 4, 2       | 12         | 1               | 1                          | 0.500      |
| 3          | 1, 1, 6       | 8          | 0               | 1                          | 0.333      |
| 4          | 5, 5, 2       | 12         | 1               | 2                          | 0.500      |
| 5          | 4, 3, 4       | 11         | 0               | 2                          | 0.400      |
| 6          | 6, 6, 1       | 13         | 1               | 3                          | 0.500      |
| 7          | 3, 2, 3       | 8          | 0               | 3                          | 0.429      |
| 8          | 6, 5, 2       | 13         | 1               | 4                          | 0.500      |
| 9          | 4, 6, 1       | 11         | 0               | 4                          | 0.444      |
| 10         | 2, 2, 6       | 10         | 0               | 4                          | 0.400      |

---

## Parte 2 ‚Äî Perguntas de interpreta√ß√£o

1. **Dire√ß√£o da estimativa.**
   Observando ($\hat p_i$) ao longo das rodadas, descreva com suas palavras o que acontece com a estimativa quando aparecem sucessos ($I_i=1$) ou fracassos ($I_i=0$). Ela parece ‚Äúoscilar‚Äù em torno de algum valor? Por qu√™?

2. **Pr√≥ximo passo (previs√£o).**
   Sem fazer uma nova simula√ß√£o, **preveja** o valor de $\hat p_{11}$) **nos dois cen√°rios**:
   a) se a pr√≥xima soma $S_{11}\ge 12$ (isto √©, $I_{11}=1$);
   b) se $S_{11}<12$ (isto √©, $I_{11}=0$).
   Dica: use ( $\hat p_{11} = \dfrac{\text{Acum. em } i=10 + I_{11}}{11}$ ).

3. **Sensibilidade da estimativa.**
   Explique por que a **mudan√ßa m√°xima** poss√≠vel em $\hat p_n$ **ap√≥s uma √∫nica nova rodada** diminui quando (n) aumenta. O que isso diz sobre a ‚Äúestabilidade‚Äù da estimativa quando fazemos muitas simula√ß√µes?

4. **Interpreta√ß√£o pr√°tica.**
   Imagine que essa probabilidade represente a chance de um **evento cr√≠tico** (por exemplo, ultrapassar um limite de risco). Como voc√™ explicaria, para algu√©m n√£o t√©cnico, por que **mais simula√ß√µes** aumentam a **confian√ßa** nessa estimativa?

---

## Parte 3 ‚Äî Continua√ß√£o (agregada) para consolidar a ideia

Ap√≥s mais **50 rodadas**, obteve-se **24 sucessos** (sem detalhar cada tripla de dados). Atualize a estimativa **global**:
$$
\hat p_{60} = \frac{\underbrace{4}*{\text{sucessos at√© } i=10} + \underbrace{24}*{\text{sucessos novos}}}{60}.
$$
Calcule $\hat p_{60}$ e compare qualitativamente com $\hat p_{10}=0{,}4$. A estimativa ficou mais est√°vel? Por qu√™?




---

# Ap√™ndice: Como calcular o seno

Vamos resolver a equa√ß√£o

$$
2\sin(x) + \sqrt{3} = 0
$$


### Passo 1: Isolar o seno

$$
2\sin(x) + \sqrt{3} = 0
$$

Subtraindo ($\sqrt{3}$) dos dois lados:

$$
2\sin(x) = -\sqrt{3}
$$

Dividindo por 2:

$$
\sin(x) = -\frac{\sqrt{3}}{2}
$$

---

### Passo 2: Encontrar o √¢ngulo de refer√™ncia

Sabemos que ($\sin(60¬∞) = \frac{\sqrt{3}}{2}$).
Portanto, o √¢ngulo de refer√™ncia √© (60¬∞).

Como o seno est√° negativo, o √¢ngulo est√° nos quadrantes III e IV, onde o seno √© menor que zero.

---

### Passo 3: Determinar as solu√ß√µes principais

No quadrante III:
$$
x_1 = 180¬∞ + 60¬∞ = 240¬∞
$$

No quadrante IV:
$$
x_2 = 360¬∞ - 60¬∞ = 300¬∞
$$

---

### Passo 4: Escrever as solu√ß√µes gerais

O seno √© peri√≥dico com per√≠odo de (360¬∞).
Logo, as solu√ß√µes gerais s√£o:

$$
x = 240¬∞ + 360¬∞k \quad \text{ou} \quad x = 300¬∞ + 360¬∞k, \quad k \in \mathbb{Z}
$$

---

### Passo 5: Conferindo em Python

```python
import math

for angulo in [240, 300]:
    print(f"sin({angulo}¬∞) =", round(math.sin(math.radians(angulo)), 3))
```

Sa√≠da:

```
sin(240¬∞) = -0.866
sin(300¬∞) = -0.866
```

Como ($\sqrt{3}/2 \approx 0.866$), o resultado confirma que

$\sin(x) = -\frac{\sqrt{3}}{2}$


![1](./plots/seno.png)
![2](./plots/seno2.png)


### Tabela do Seno 

| √Çngulo (¬∞) | √Çngulo (rad) | sen(Œ∏) exato / aproximado |
| ---------- | ------------ | ------------------------- |
| 0¬∞         | 0            | 0.0000                    |
| 15¬∞        | œÄ/12         | 0.2588                    |
| 30¬∞        | œÄ/6          | 0.5000                    |
| 45¬∞        | œÄ/4          | 0.7071                    |
| 60¬∞        | œÄ/3          | 0.8660                    |
| 75¬∞        | 5œÄ/12        | 0.9659                    |
| 90¬∞        | œÄ/2          | 1.0000                    |
| 105¬∞       | 7œÄ/12        | 0.9659                    |
| 120¬∞       | 2œÄ/3         | 0.8660                    |
| 135¬∞       | 3œÄ/4         | 0.7071                    |
| 150¬∞       | 5œÄ/6         | 0.5000                    |
| 165¬∞       | 11œÄ/12       | 0.2588                    |
| 180¬∞       | œÄ            | 0.0000                    |
| 195¬∞       | 13œÄ/12       | -0.2588                   |
| 210¬∞       | 7œÄ/6         | -0.5000                   |
| 225¬∞       | 5œÄ/4         | -0.7071                   |
| 240¬∞       | 4œÄ/3         | -0.8660                   |
| 255¬∞       | 17œÄ/12       | -0.9659                   |
| 270¬∞       | 3œÄ/2         | -1.0000                   |
| 285¬∞       | 19œÄ/12       | -0.9659                   |
| 300¬∞       | 5œÄ/3         | -0.8660                   |
| 315¬∞       | 7œÄ/4         | -0.7071                   |
| 330¬∞       | 11œÄ/6        | -0.5000                   |
| 345¬∞       | 23œÄ/12       | -0.2588                   |
| 360¬∞       | 2œÄ           | 0.0000                    |



# Intui√ß√£o Gradiente Descendente

Vamos aprofundar essa ideia ‚Äî o **sinal do gradiente** √©, de fato, o que indica **a dire√ß√£o da descida**. Abaixo vai uma explica√ß√£o intuitiva e depois uma mais formal.

---

## 1. Intui√ß√£o: ‚Äúsubida‚Äù e ‚Äúdescida‚Äù no terreno

Imagine que (J(w)) √© uma **colina** e voc√™ est√° nela, tentando chegar ao ponto mais baixo.
O **gradiente** √© uma ‚Äúb√∫ssola‚Äù que aponta **para o lado da subida mais √≠ngreme**.

* Se o **gradiente √© positivo**, quer dizer que, se voc√™ andar para a direita, **a altura aumenta**.
  Logo, o caminho para **descer** √© o **oposto**, ou seja, andar para a **esquerda**.
* Se o **gradiente √© negativo**, isso indica que, ao andar para a direita, **a altura diminui**.
  Ent√£o, o caminho de descida √© para a **direita** (seguir aumentando (w)).

Por isso a f√≥rmula do gradiente descendente tem um **sinal de menos**:
$$
w_{t+1} = w_t - \eta \nabla J(w_t)
$$
Ela garante que voc√™ sempre anda **na dire√ß√£o contr√°ria ao gradiente**, ou seja, sempre descendo a montanha.

---

## 2. Visualizando o sentido do movimento

Pense na fun√ß√£o $J(w) = (w - 3)^2 + 2$

| Regi√£o  | Sinal do gradiente | Efeito                                                                                |
| ------- | ------------------ | ------------------------------------------------------------------------------------- |
| (w < 3) | negativo           | estamos √† esquerda do m√≠nimo ‚Üí fun√ß√£o desce se formos para a direita ‚Üí aumentamos (w) |
| (w > 3) | positivo           | estamos √† direita do m√≠nimo ‚Üí fun√ß√£o desce se formos para a esquerda ‚Üí diminu√≠mos (w) |
| (w = 3) | zero               | ponto de equil√≠brio ‚Üí nenhum movimento                                                |

Assim, o algoritmo naturalmente ‚Äúpuxa‚Äù (w) em dire√ß√£o a 3, porque cada passo sempre aponta para o centro da par√°bola.

---

## 3. Rela√ß√£o com a f√≥rmula

No passo:
$$
w_{t+1} = w_t - \eta , \nabla J(w_t)
$$

* Se ($\nabla J(w_t) > 0$) ‚Üí o termo ($-\eta \nabla J(w_t)$) √© **negativo**, logo ($w_{t+1} < w_t$): andamos **para a esquerda**.
* Se ($\nabla J(w_t) < 0$) ‚Üí o termo ($-\eta \nabla J(w_t)$) √© **positivo**, logo ($w_{t+1} > w_t$): andamos **para a direita**.

Ou seja:

> O **sinal do gradiente** diz se estamos √† direita ou √† esquerda do m√≠nimo,
> e o **sinal de menos** na f√≥rmula garante que o movimento sempre v√° em dire√ß√£o ao ponto mais baixo.

![](./plots/grad_desc2.png)
O gr√°fico com **setas verdes** mostrando a **dire√ß√£o do movimento**:

* √Ä **esquerda de (w=3)** (onde o gradiente √© negativo), as setas apontam **para a direita** ‚Äî o algoritmo aumenta (w).
* √Ä **direita de (w=3)** (onde o gradiente √© positivo), as setas apontam **para a esquerda** ‚Äî o algoritmo diminui (w).
* No **ponto (w=3)**, n√£o h√° seta: o gradiente √© zero e o algoritmo para ‚Äî chegamos ao **m√≠nimo**.

Esse √© exatamente o comportamento do gradiente descendente: **seguir sempre na dire√ß√£o oposta √† inclina√ß√£o** at√© atingir o ponto mais baixo da curva.

Provas r√°pidas:

1. Caso (w<3) (ex.: (w=2{,}5))
   $\nabla J(w)=2(w-3) < 0$.
   Teste local: $J(2{,}6)=(2{,}6-3)^2+2=2{,}16 < J(2{,}5)=2{,}25$. A fun√ß√£o **desce** quando **aumentamos** $w$ Logo, movemos para a direita.

2. Caso (w>3) (ex.: (w=3{,}5))
   $\nabla J(w)=2(w-3) > 0$
   Teste local: $J(3{,}4)=(3{,}4-3)^2+2=2{,}16 < J(3{,}5)=2{,}25$ A fun√ß√£o **desce** quando **diminu√≠mos** (w). Logo, movemos para a esquerda.

3. Caso (w=3)
   $\nabla J(w)=0$. N√£o h√° dire√ß√£o de descida: √© o m√≠nimo.

Outra forma de ver: minimizar (J(w)=(w-3)^2+2) √© o mesmo que **reduzir (|w-3|)**.

* Se (w<3), aumentar (w) reduz (|w-3|).
* Se (w>3), diminuir (w) reduz (|w-3|).

E pela atualiza√ß√£o (w_{t+1}=w_t-\eta,\nabla J(w_t)):

* Se $nabla J<0$, ent√£o $-\eta,\nabla J>0$ e (w) **aumenta**.
* Se $nabla J>0$, ent√£o $-\eta,\nabla J<0$ e (w) **diminui**.

Portanto, a leitura da sua tabela est√° certa:

* (w<3) ‚Üí gradiente negativo ‚Üí andar para a **direita** (aumentar (w));
* (w>3) ‚Üí gradiente positivo ‚Üí andar para a **esquerda** (diminuir (w));
* (w=3) ‚Üí gradiente zero ‚Üí **parar**.

